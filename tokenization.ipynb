{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae93f749",
   "metadata": {},
   "source": [
    "### Tokeniztion is the process of splitting the text into meaningfull segments\n",
    "\n",
    "##### we can't use (.) for splitting the sentences or words we need to have an info about the rules of the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01dc093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2e411",
   "metadata": {},
   "source": [
    "#### here we just create a blank pipeline of the english language\n",
    "blank pipe line will return an obj that just know about tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d057b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe38e1",
   "metadata": {},
   "source": [
    "# Tokenizier\n",
    "<p style = \"color : red\"> First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.<\\p>\n",
    "\n",
    "### Spacy first split on the basis of the prefix than it's look for exception  and create tokens and at the end it will look suffix and if there is any exception it will handle it too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8651ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Let's go to N.Y!\")\n",
    "for sentense in doc:\n",
    "    print(sentense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0084f6",
   "metadata": {},
   "source": [
    "### It can differentitate between currency and others strings two just look it split 2 $ into two seperate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a06c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Seher\n",
      "is\n",
      "good\n",
      ".\n",
      "She\n",
      "likes\n",
      "burgers\n",
      "and\n",
      "biryani\n",
      ".\n",
      "The\n",
      "price\n",
      "was\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Seher is good. She likes burgers and biryani. The price was 2$ per plate.\")\n",
    "for word in doc:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fc7e5",
   "metadata": {},
   "source": [
    "#### we can perform string operation on the obj of nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11b6f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60eeed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1 return the last element of the array\n",
    "doc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747cb38",
   "metadata": {},
   "source": [
    "## look at the type of the nlp and doc \n",
    "nlp is the obj of english language\n",
    "and doc is type of span which means that the a collection of sub strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6cbf976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2313e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"Mark give 5 $ for the shopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6ed4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mark"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc1[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237f04d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a93d3f",
   "metadata": {},
   "source": [
    "# Attribute of the Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a8810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ef0bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c426a864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3 = doc1[3]\n",
    "token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ef02a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58cd8285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. =====> index 0 \n",
      " is num:  False \n",
      " is aphabet :  False\n",
      "Seher =====> index 1 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "is =====> index 2 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "good =====> index 3 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      ". =====> index 4 \n",
      " is num:  False \n",
      " is aphabet :  False\n",
      "She =====> index 5 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "likes =====> index 6 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "burgers =====> index 7 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "and =====> index 8 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "biryani =====> index 9 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      ". =====> index 10 \n",
      " is num:  False \n",
      " is aphabet :  False\n",
      "The =====> index 11 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "price =====> index 12 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "was =====> index 13 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "2 =====> index 14 \n",
      " is num:  False \n",
      " is aphabet :  False\n",
      "$ =====> index 15 \n",
      " is num:  True \n",
      " is aphabet :  False\n",
      "per =====> index 16 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      "plate =====> index 17 \n",
      " is num:  False \n",
      " is aphabet :  True\n",
      ". =====> index 18 \n",
      " is num:  False \n",
      " is aphabet :  False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"=====>\", \"index\", token.i,\n",
    "         \"\\n is num: \", token.is_currency,\n",
    "         \"\\n is aphabet : \", token.is_alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15b28266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4861cd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e212f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for text in doc:\n",
    "    if text.like_email:\n",
    "        emails.append(text)\n",
    "emails        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6413b",
   "metadata": {},
   "source": [
    "# sometime we want to customize the tokenization so we use ORTH fn of spacy\n",
    "**In some cases, we need to customize our tokenization rules. This could be an expression or \n",
    "an abbreviation for example if we have ViratKohli in our text and we need to split it (Virat Kholi) during \n",
    "tokenization. This could be done by \n",
    "adding a special rule to an existing Tokenizer rule.**\n",
    "\n",
    "\n",
    "IN the below example, we need to split gimme into two tokens so we will write custom code for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bb2dbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gimme, double, cheese, extra, large, healthy, pizza]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fed14a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[gim, me, double, cheese, extra, large, healthy, pizza]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case('gimme', [\n",
    "    {ORTH : \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519a5f5",
   "metadata": {},
   "source": [
    "# here we create a blank pipeline we can add different things in our pipeline according to the requirements     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ec9f3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ede352",
   "metadata": {},
   "source": [
    "# Adding an element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a57d1355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1c50c83ab40>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a01ce817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for token in doc.sents:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30673480",
   "metadata": {},
   "source": [
    "### Now it's have one element in the nlp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2618979f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x1c50c83ab40>)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
